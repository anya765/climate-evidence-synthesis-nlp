{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOq2VOchi7oZfWelCei8Sfs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anya765/climate-evidence-synthesis-nlp/blob/main/geoparse.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NHCtLneNmyLC"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import os\n",
        "\n",
        "if os.path.exists(\"/content/\"):\n",
        "    from google.colab import drive\n",
        "    import os\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    os.chdir(\"/content/drive/MyDrive/climate-science\") \n",
        "\n",
        "from run_cv_experiments import load_data\n",
        "\n",
        "df = load_data(False)\n",
        "df.loc[pd.isna(df[\"id\"]), \"id\"] = df.loc[pd.isna(df[\"id\"]), \"OA_id\"]\n",
        "\n",
        "#Merge data with predictions\n",
        "df = df.merge(pd.read_csv('cv_data/INCLUDE/predictions_5_splits.csv'), how=\"outer\")\n",
        "\n",
        "#if no data- put in diff column\n",
        "df.loc[pd.isna(df[\"INCLUDE_prediction\"]),\"INCLUDE_prediction\"] = df.loc[pd.isna(df[\"INCLUDE_prediction\"]),\"INCLUDE\"]\n",
        "print(df.shape)\n",
        "df = df[(df[\"INCLUDE_prediction\"]>=0.5)]\n",
        "print(df.shape)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#load any combined_place_df that have already been processed or initialise an empty dataframe\n",
        "if os.path.exists(\"data/combined_place_df.csv\"):\n",
        "    processed_place_df = pd.read_csv(\"data/combined_place_df.csv\")\n",
        "    unprocessed_place_df = df[~df['id'].isin(processed_place_df)]\n",
        "else:\n",
        "    processed_place_df = pd.DataFrame()\n",
        "    unprocessed_place_df = df"
      ],
      "metadata": {
        "id": "-hVk4mg1m5k4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install mordecai\n",
        "! python -m spacy download en_core_web_lg\n",
        "! python -m spacy download en\n",
        "! python â€“m spacy info"
      ],
      "metadata": {
        "id": "bAcnWn-ufn_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load('en')\n",
        "from mordecai import Geoparser\n",
        "geo = Geoparser()\n",
        "geo.geoparse(\"I travelled from Oxford to Ottawa\")"
      ],
      "metadata": {
        "id": "JebqyS4Bm7is"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture \n",
        "places = []\n",
        "geos = []\n",
        "\n",
        "import re\n",
        "\n",
        "#loop through rows in dataframe\n",
        "for i, row in unprocessed_place_df.iterrows():\n",
        "    \n",
        "    #add text to geoparse, join title and abstract, get rid of copyright stuff\n",
        "    t = str(row['title']) + \" \" + str(row['abstract'])\n",
        "    t = t.split(\"Copyright (C)\")[0] \n",
        "    t = re.split(\"\\([C-c]\\) [1-2][0-9]{3} Elsevier\",t)[0] \n",
        "    t = t.split(\"Published by Elsevier\")[0] \n",
        "    t = t.split(\"Copyright. (C)\")[0] \n",
        "    t = re.split(\"\\. \\(C\\) [1-2][0-9]{3} \",t)[0] \n",
        "    t = re.split(\"\\. \\(C\\) Copyright\",t)[0]   \n",
        "    t = re.split(\"\\. \\\\xA9 [1-2][0-9]{3}\", t)[0] #Copyright symbol\n",
        "    \n",
        "    #remove common place names involved in environmental studies\n",
        "    t = re.sub(\"paris agreement\", \"\", t, flags=re.I)\n",
        "    t = re.sub(\"kyoto protocol\", \"\", t, flags=re.I)\n",
        "    t = re.sub(\"montreal protocol\", \"\", t, flags=re.I)\n",
        "    t = re.sub(\"london protocol\", \"\", t, flags=re.I)\n",
        "    \n",
        "    #geoparse yay\n",
        "    gp = geo.geoparse(t)\n",
        "    \n",
        "    #for each place, append to a list of dictionaries, with a field for the doc_id\n",
        "    for p in gp:\n",
        "        if \"geo\" in p:\n",
        "            for key, value in p[\"geo\"].items():\n",
        "                p[key] = value\n",
        "            del p[\"geo\"]\n",
        "            \n",
        "        p[\"doc_id\"] = row[\"id\"]\n",
        "        places.append(p)\n",
        "\n",
        "    #save every thousand rows - helps to not start again if there's interruptions\n",
        "    if i % 1000 == 0:\n",
        "        combined_place_df = processed_place_df.append(pd.DataFrame.from_dict(places))\n",
        "        print(combined_place_df.shape)\n",
        "        combined_place_df.to_csv(\"data/combined_place_df.csv\", index=False)\n",
        "    \n",
        "#mergeeeeeeeeeeee \n",
        "combined_place_df = processed_place_df.append(pd.DataFrame.from_dict(places))\n",
        "print(combined_place_df.shape)\n",
        "combined_place_df.to_csv(\"data/combined_place_df.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "9UAwOpD5m_gZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['tstring'] = df['title'] + \" \" + df['abstract']\n",
        "\n",
        "gm_docs = df.loc[\n",
        "    (df['tstring'].str.lower().str.contains(\"gulf of mexico\")),\n",
        "    \"id\"\n",
        "]\n",
        "geocolumns = [\"word\", \"country_conf\", \"feature_code\",\"lat\",\"lon\",\"place_name\",\"feature_class\",\"geonameid\"]\n",
        "gm = pd.DataFrame({\"doc_id\": gm_docs})\n",
        "gm[geocolumns] = [\"Gulf of Mexico\",0.8,\"GULF\", 25, -90, \"Gulf of Mexico\", \"H\", 3523271]\n",
        "\n",
        "combined_place_df = pd.concat([combined_place_df, gm])\n",
        "\n",
        "\n",
        "lab_docs = df.loc[\n",
        "    (df['tstring'].str.lower().str.contains(\"labrador sea\")),\n",
        "    \"id\"\n",
        "]\n",
        "geocolumns = [\"word\", \"country_conf\", \"feature_code\",\"lat\",\"lon\",\"place_name\",\"feature_class\",\"geonameid\"]\n",
        "lab = pd.DataFrame({\"doc_id\": lab_docs})\n",
        "lab[geocolumns] = [\"Labrador Sea\",0.8,\"SEA\", 57, -55, \"Labrador Sea\", \"H\", 3424929]\n",
        "\n",
        "combined_place_df = pd.concat([combined_place_df, lab])\n",
        "\n",
        "baf_docs = df.loc[\n",
        "    (df['tstring'].str.lower().str.contains(\"baffin bay\")),\n",
        "    \"id\"\n",
        "]\n",
        "geocolumns = [\"word\", \"country_conf\", \"feature_code\",\"lat\",\"lon\",\"place_name\",\"feature_class\",\"geonameid\"]\n",
        "baf = pd.DataFrame({\"doc_id\": baf_docs})\n",
        "baf[geocolumns] = [\"Baffin Bay\",0.8,\"BAY\", 74, -68, \"Baffin Bay\", \"H\", 3831554]\n",
        "combined_place_df = pd.concat([combined_place_df, baf])\n",
        "\n",
        "\n",
        "ok_docs = df.loc[\n",
        "    (df['tstring'].str.lower().str.contains(\"sea of okhotsk\")) ,\n",
        "    \"id\"\n",
        "]\n",
        "geocolumns = [\"word\", \"country_conf\", \"feature_code\",\"lat\",\"lon\",\"place_name\",\"feature_class\",\"geonameid\"]\n",
        "ok = pd.DataFrame({\"doc_id\": ok_docs})\n",
        "ok[geocolumns] = [\"Sea of Okhotsk\",0.8, \"SEA\", 55, 150, \"Sea of Okhotsk\", \"H\", 2127380]\n",
        "combined_place_df = pd.concat([combined_place_df, ok])\n",
        "\n",
        "\n",
        "kyoto_docs = df.loc[\n",
        "    (df['tstring'].str.lower().str.contains(\"kyoto target\")) |\n",
        "    (df['tstring'].str.lower().str.contains(\"kyoto process\")) |\n",
        "    (df['tstring'].str.lower().str.contains(\"kyoto emission\")) |\n",
        "    (df['tstring'].str.lower().str.contains(\"kyoto gas\")) |\n",
        "    (df['tstring'].str.lower().str.contains(\"kyoto agreement\")) |\n",
        "    (df['tstring'].str.lower().str.contains(\"kyoto protocol\")) |\n",
        "    (df['tstring'].str.lower().str.contains(\"kyoto framework\")),\n",
        "    \"id\"\n",
        "]\n",
        "\n",
        "combined_place_df = combined_place_df.drop(combined_place_df[(combined_place_df['doc_id'].isin(kyoto_docs)) & (combined_place_df['word'].str.lower()==\"kyoto\")].index)\n",
        "\n",
        "paris_docs = df.loc[\n",
        "    (df['tstring'].str.contains('(Paris(?:\\S* ){0,15}COP)|(COP(?:\\S* ){0,15}Paris)')) |\n",
        "    (df['tstring'].str.contains('(Paris(?:\\S* ){0,15}Agreement)|(COP(?:\\S* ){0,15}Agreement)')) ,\n",
        "    'id'\n",
        "]\n",
        "combined_place_df = combined_place_df.drop(combined_place_df[(combined_place_df['doc_id'].isin(paris_docs)) & (combined_place_df['word'].str.lower()==\"paris\")].index)\n",
        "\n",
        "#Copenhagen\n",
        "copenhagen_docs = df.loc[\n",
        "    (df['tstring'].str.contains('(Copenhagen(?:\\S* ){0,15}COP)|(COP(?:\\S* ){0,15}Copenhagen)')) |\n",
        "    (df['tstring'].str.contains('(Copenhagen(?:\\S* ){0,3}Accord)|(Accord(?:\\S* ){0,3}Copenhagen)')) ,\n",
        "    'id'\n",
        "]\n",
        "combined_place_df = combined_place_df.drop(combined_place_df[(combined_place_df['doc_id'].isin(copenhagen_docs)) & (combined_place_df['word'].str.lower()==\"copenhagen\")].index)\n",
        "\n",
        "#Berlin\n",
        "berlin_docs = df.loc[\n",
        "    (df['tstring'].str.contains('(Berlin(?:\\S* ){0,15}COP)|(COP(?:\\S* ){0,15}Berlin)')),\n",
        "    'id'\n",
        "]\n",
        "combined_place_df = combined_place_df.drop(combined_place_df[(combined_place_df['doc_id'].isin(berlin_docs)) & (combined_place_df['word'].str.lower()==\"berlin\")].index)\n",
        "\n",
        "#Glasgow\n",
        "berlin_docs = df.loc[\n",
        "    (df['tstring'].str.contains('(Glasgow(?:\\S* ){0,15}COP)|(COP(?:\\S* ){0,15}Glasgow)')),\n",
        "    'id'\n",
        "]\n",
        "combined_place_df = combined_place_df.drop(combined_place_df[(combined_place_df['doc_id'].isin(berlin_docs)) & (combined_place_df['word'].str.lower()==\"berlin\")].index)\n",
        "\n",
        "#Cancun\n",
        "cancun_docs = df.loc[\n",
        "    (df['tstring'].str.contains('(Cancun(?:\\S* ){0,15}COP)|(COP(?:\\S* ){0,15}Cancun)')) |\n",
        "    (df['tstring'].str.lower().str.contains('cancun pledge')),\n",
        "    'id'\n",
        "]\n",
        "combined_place_df = combined_place_df.drop(combined_place_df[(combined_place_df['doc_id'].isin(cancun_docs)) & (combined_place_df['word'].str.lower()==\"cancun\")].index)\n",
        "\n",
        "\n",
        "geocolumns = [\"feature_code\", \"lat\", \"lon\", \"place_name\", \"feature_class\", \"geonameid\", \"country_code3\"]\n",
        "\n",
        "combined_place_df.loc[combined_place_df[\"word\"]==\"Pakistan\", geocolumns]=[\"PCLI\",30,70,\"Islamic Republic of Pakistan\",\"A\",1168579,\"PAK\"]\n",
        "combined_place_df.loc[combined_place_df[\"word\"]==\"Colombia\", geocolumns]=[\"PCLI\",4,-73.25,\"Colombia\",\"A\",3686110, \"COL\"]\n",
        "combined_place_df.loc[combined_place_df[\"word\"]==\"Argentina\", geocolumns]=[\"PCLI\",-34,-64,\"Argentine Republic\",\"A\",3865483, \"ARG\"]\n",
        "combined_place_df.loc[combined_place_df[\"word\"]==\"Sahara\", geocolumns] = [\"DSRT\", 26, 13, \"Sahara\", \"T\", 2212709, None]\n",
        "combined_place_df.loc[combined_place_df[\"word\"]==\"Alps\",geocolumns] = [\"MTS\", 46.41667, 10, \"Alps\", \"T\", 2661786, None]\n",
        "combined_place_df.loc[combined_place_df[\"word\"]==\"Mediterranean Sea\",geocolumns] = [\"SEA\", 35, 20, \"Mediterranean Sea\", \"T\", 2661786, None]\n",
        "combined_place_df.loc[combined_place_df[\"word\"]==\"MEDITERRANEAN\",geocolumns] = [\"SEA\", 35, 20, \"Mediterranean Sea\", \"T\", 2661786, None]\n",
        "combined_place_df.loc[combined_place_df[\"word\"]==\"East China\",geocolumns] = [\"PCLI\", 35, 105, \"China\", \"A\", 1814991, \"CHN\"]\n",
        "combined_place_df.loc[combined_place_df[\"word\"]==\"South China\",geocolumns] = [\"PCLI\", 35, 105, \"China\", \"A\", 1814991, \"CHN\"]\n",
        "combined_place_df.loc[combined_place_df[\"word\"]==\"Great Lakes\",geocolumns] = [\"LK\", 45.68751, -84.43753, \"Great Lakes\", \"H\", 4994594, \"USA\"]\n",
        "combined_place_df.loc[combined_place_df[\"word\"]==\"Catalonia\",geocolumns] = [\"ADM1\", 41.82046, 1.86768, \"Catalunya\", \"A\", 3336901, \"ESP\"]\n",
        "combined_place_df.loc[combined_place_df[\"word\"]==\"South Pacific\",geocolumns] = [\"OCN\", -45, -130, \"South Pacific Ocean\", \"H\", 4030483, None]\n",
        "combined_place_df.loc[combined_place_df[\"word\"]==\"Gulf Coast\",geocolumns] = [\"AREA\", 29.36901, -95.00565, \"Gulf Coast\", \"L\", 7287689, \"USA\"]\n",
        "combined_place_df.loc[combined_place_df[\"word\"]==\"Gulf coast\",geocolumns] = [\"AREA\", 29.36901, -95.00565, \"Gulf Coast\", \"L\", 7287689, \"USA\"]\n",
        "combined_place_df.loc[combined_place_df[\"word\"]==\"Hainan Island\",geocolumns] = [\"ISL\", 19.2, 109.7, \"Hainan Dao\", \"T\", 1809055, \"CHN\"]\n",
        "combined_place_df.loc[combined_place_df[\"word\"]==\"Red Sea\",geocolumns] = [\"SEA\", 20.26735, 38.53455, \"Red Sea\", \"H\", 350155, None]\n",
        "combined_place_df.loc[combined_place_df[\"word\"]==\"Himalayan\",geocolumns] = [\"MTS\", 28,84, \"Himalayas\", \"T\", 1252558, None]\n",
        "combined_place_df.loc[combined_place_df[\"word\"]==\"Himalayas\",geocolumns] = [\"MTS\", 28,84, \"Himalayas\", \"T\", 1252558, None]\n",
        "combined_place_df.loc[combined_place_df[\"word\"]==\"North America's\",geocolumns] = [\"CONT\", 46.07323, -100.54688, \"North America\", \"L\", 6255149, None]\n",
        "combined_place_df.loc[combined_place_df[\"word\"]==\"Atlantic Ocean\",geocolumns] = [\"OCN\", 10, -25, \"Atlantic Ocean\", \"H\", 3373405, None]\n",
        "combined_place_df.loc[combined_place_df[\"word\"]==\"Scandinavia\",geocolumns] = [\"RGN\", 63, 12, \"Scandinavia\", \"L\", 2614165, None]\n",
        "combined_place_df.loc[combined_place_df[\"word\"]==\"California (USA\",geocolumns] = [\"ADM1\", 37.25022, -119.75126, \"California\", \"A\", 5332921, \"USA\"]\n",
        "combined_place_df.loc[combined_place_df[\"word\"]==\"California, USA\",geocolumns] = [\"ADM1\", 37.25022, -119.75126, \"California\", \"A\", 5332921, \"USA\"]\n",
        "combined_place_df.loc[combined_place_df[\"word\"]==\"North Pacific\",geocolumns] = [\"OCN\", 30, -170, \"North Pacific Ocean\", \"H\", 4030875, None]\n",
        "combined_place_df.loc[combined_place_df[\"word\"]==\"Huai\",geocolumns] = [\"STM\", 33.133333, 118.5, \"Huai He\", \"H\", 1807690, \"CHN\"]\n",
        "combined_place_df.loc[combined_place_df[\"word\"]==\"Washington, DC\",geocolumns] = [\"PPLC\", 38.89511, -77.03637, \"Washington\", \"P\", 4140963, \"USA\"]\n",
        "combined_place_df.loc[combined_place_df[\"word\"]==\"Messinian\",geocolumns] = [\"ADM2\", 37.25, -21.83333, \"Nomos Messinias\", \"A\", 257149, \"GRC\"]\n",
        "combined_place_df.loc[combined_place_df[\"word\"]==\"Ionian Sea\",geocolumns] = [\"SEA\", 39, 19, \"Ionian Sea\", \"H\", 2463713, None]\n",
        "combined_place_df.loc[combined_place_df[\"word\"]==\"NYC\",geocolumns] = [\"PPL\", 40.71427, -74.00597, \"New York City\", \"P\", 5128581, \"USA\"]\n",
        "combined_place_df.loc[combined_place_df[\"word\"]==\"Indian Ocean\",geocolumns] = [\"OCN\", -10, 70, \"Indian Ocean\", \"P\", 1545739, None]\n",
        "combined_place_df.loc[combined_place_df[\"word\"]==\"North Sea\",geocolumns] = [\"SEA\", 55, 3, \"North Sea\", \"P\", 2960848, None]\n",
        "combined_place_df.loc[combined_place_df[\"word\"]==\"Philippine Sea\",geocolumns] = [\"SEA\", 20, 135, \"Philippine Sea\", \"P\", 1818190, None]\n",
        "combined_place_df.loc[combined_place_df[\"word\"]==\"Black Sea\",geocolumns] = [\"SEA\", 43, 34, \"Black Sea\", \"H\", 630673, None]\n",
        "combined_place_df.loc[combined_place_df[\"word\"]==\"Coral Sea\",geocolumns] = [\"SEA\", -20, 155, \"Coral Sea\", \"H\", 2194166, None]\n",
        "combined_place_df.loc[combined_place_df[\"word\"]==\"Timor Sea\",geocolumns] = [\"SEA\", -11, 127, \"Timor Sea\", \"H\", 2078065, None]\n",
        "combined_place_df.loc[combined_place_df[\"word\"]==\"Hudson Bay\",geocolumns] = [\"BAY\", 60, -85, \"Hudson Bay\", \"H\", 5978134, \"CAN\"]\n",
        "combined_place_df.loc[combined_place_df[\"word\"]==\"Bering Sea\",geocolumns] = [\"SEA\", 60, -175, \"Bering Sea\", \"H\", 4031788, None]\n",
        "combined_place_df.loc[combined_place_df[\"word\"]==\"Okhotsk Sea\",geocolumns] = [\"SEA\", 55, 150, \"Sea of Okhotsk\", \"H\", 2127380, None]\n",
        "\n",
        "combined_place_df.loc[combined_place_df[\"place_name\"]==\"Central Upper Nile\",geocolumns] = [\"ADM1\", 10, 32.7, \"Upper Nile\", \"A\", 381229, \"SSD\"]\n",
        "combined_place_df.loc[combined_place_df[\"place_name\"]==\"Gobolka Woqooyi Galbeed\",\"place_name\"] = \"Woqooyi Galbeed\"\n",
        "\n",
        "combined_place_df = combined_place_df[combined_place_df[\"place_name\"]!=\"Pacific County\"]\n",
        "combined_place_df = combined_place_df.loc[combined_place_df[\"word\"]!=\"B.V.\"]\n",
        "combined_place_df = combined_place_df[combined_place_df[\"word\"]!=\"MMT\"]\n",
        "combined_place_df = combined_place_df[combined_place_df[\"word\"]!=\"Yellow\"]\n",
        "combined_place_df = combined_place_df[combined_place_df[\"word\"]!=\"Hadley\"]\n",
        "combined_place_df = combined_place_df[combined_place_df[\"word\"]!=\"Western North\"]\n",
        "combined_place_df = combined_place_df[combined_place_df[\"word\"]!=\"colonies\"]\n",
        "combined_place_df = combined_place_df[combined_place_df[\"word\"]!=\"TN\"]\n",
        "combined_place_df = combined_place_df[combined_place_df[\"word\"]!=\"NH\"]\n",
        "combined_place_df = combined_place_df[combined_place_df[\"word\"]!=\"Mn\"]\n",
        "combined_place_df = combined_place_df[combined_place_df[\"word\"]!=\"Tx\"]\n",
        "combined_place_df = combined_place_df[combined_place_df[\"word\"]!=\"TX\"]\n",
        "combined_place_df = combined_place_df[combined_place_df[\"word\"]!=\"Tn\"]\n",
        "combined_place_df = combined_place_df[combined_place_df[\"word\"]!=\"FL\"]\n",
        "combined_place_df = combined_place_df[combined_place_df[\"word\"]!=\"Spartina\"]\n",
        "combined_place_df = combined_place_df[combined_place_df[\"word\"]!=\"Tamarix\"]\n",
        "combined_place_df = combined_place_df[combined_place_df[\"word\"]!=\"Eurasia\"]\n",
        "combined_place_df = combined_place_df[combined_place_df[\"word\"]!=\"Phillyrea\"]\n",
        "combined_place_df = combined_place_df[combined_place_df[\"word\"]!=\"N-15\"]\n",
        "combined_place_df = combined_place_df[combined_place_df[\"word\"]!=\"LT50\"]\n",
        "combined_place_df = combined_place_df[combined_place_df[\"word\"]!=\"POSEIDON\"]\n",
        "combined_place_df = combined_place_df[combined_place_df[\"word\"]!=\"LC50\"]\n",
        "combined_place_df = combined_place_df[combined_place_df[\"word\"]!=\"El Nio\"]\n",
        "combined_place_df = combined_place_df[combined_place_df[\"word\"]!=\"La Nia\"]\n",
        "combined_place_df = combined_place_df[combined_place_df[\"word\"]!=\"Red\"]\n",
        "combined_place_df = combined_place_df[combined_place_df[\"word\"]!=\"Gulf Stream\"]\n",
        "combined_place_df = combined_place_df[combined_place_df[\"word\"].str.len()>2]\n",
        "combined_place_df = combined_place_df[combined_place_df[\"word\"]!=\"NH 1\"]\n",
        "combined_place_df = combined_place_df[combined_place_df[\"word\"]!=\"Quercus\"]\n",
        "\n",
        "\n",
        "combined_place_df = combined_place_df[(combined_place_df[\"word\"]!=\"ZJP\")]\n",
        "combined_place_df = combined_place_df[(combined_place_df[\"word\"]!=\"MSW\")]\n",
        "combined_place_df = combined_place_df[(combined_place_df[\"word\"]!=\"CCS\")]\n",
        "combined_place_df = combined_place_df[(combined_place_df[\"word\"]!=\"Tier-3\")]\n",
        "combined_place_df = combined_place_df[(combined_place_df[\"word\"]!=\"N2O\")]\n",
        "combined_place_df = combined_place_df[(combined_place_df[\"word\"]!=\"VKT\")]\n",
        "combined_place_df = combined_place_df[(combined_place_df[\"word\"]!=\"OECD\")]\n",
        "combined_place_df = combined_place_df[(combined_place_df[\"word\"]!=\"States\")]\n",
        "combined_place_df = combined_place_df[(combined_place_df[\"word\"]!=\"North to South\")]\n",
        "combined_place_df = combined_place_df[(combined_place_df[\"word\"]!=\"Stabilising\")]\n",
        "combined_place_df = combined_place_df[(combined_place_df[\"word\"]!=\"Mass Railway\")]\n",
        "combined_place_df = combined_place_df[(combined_place_df[\"word\"]!=\"City\")]\n",
        "\n",
        "combined_place_df.loc[combined_place_df[\"word\"]==\"Ireland\", geocolumns]=[\"PCLI\",53,-8,\"Ireland\",\"A\",2963597,\"IRL\"]\n",
        "combined_place_df.loc[combined_place_df[\"word\"]==\"United States\", geocolumns] = [\"PCLI\",39.76,-98.5,\"United States\",\"A\",6252001, \"USA\"]\n",
        "combined_place_df.loc[combined_place_df[\"word\"]==\"Czech Republic\", geocolumns] = [\"PCLI\",49.75,15,\"Czechia\",\"A\",3077311, \"CZE\"]\n",
        "combined_place_df.loc[combined_place_df[\"word\"]==\"Czechia\", geocolumns] = [\"PCLI\",49.75,15,\"Czechia\",\"A\",3077311, \"CZE\"]\n",
        "combined_place_df.loc[combined_place_df[\"word\"]==\"China\", geocolumns] = [\"PCLI\", 35, 105, \"China\", \"A\", 1814991, \"CHN\"]\n",
        "combined_place_df.loc[combined_place_df[\"word\"]==\"United Arab Emirates\", geocolumns] = [\"PCLI\", 23.75, 54.5, \"United Arab Emirates\", \"A\", 290557, \"ARE\"]\n",
        "\n",
        "\n",
        "combined_place_df.to_csv('data/places.csv', index=False)\n",
        "\n",
        "print(combined_place_df.shape)\n",
        "\n",
        "combined_place_df.tail()"
      ],
      "metadata": {
        "id": "vd58wrkxnG_n"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}